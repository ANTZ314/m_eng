# -*- coding: utf-8 -*-
"""vit_custom06.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y_rmDevtQgugrKic6C_gArvEeSq9tfL5

# From ViT_06

**Description**:
* Dataset: cifar100 [32, 32, 3]
* Experiment_1 - Vanilla ViT Model
* Experiment_2 - Implement ViT with Shifted Patch Tokenization & Locality Self Attention
[LINK](https://keras.io/examples/vision/vit_small_ds/)

**Requires**: 
* TensorFlow 2.6 or higher
* TensorFlow Addons:
"""

!pip install -qq -U tensorflow-addons

import math
import glob
import numpy as np
import tensorflow as tf
from tensorflow import keras
import tensorflow_addons as tfa
import matplotlib.pyplot as plt
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

# Setting seed for reproducibiltiy
SEED = 42
keras.utils.set_random_seed(SEED)

"""## Get & Prepare Dataset"""

from google.colab import drive
drive.mount('/content/gdrive')

# Check dataset avaiabe
!ls /content/gdrive/MyDrive/dataset/

# Copy Kaggle Leather Dataset to current directory
!cp /content/gdrive/MyDrive/dataset/new_kaggle.tar.xz .

# Create dataset destination
!mkdir /content/leather

# Extract to new direcotry
!tar -xf  'new_kaggle.tar.xz' -C '/content/leather/'

!ls /content/leather/new_kaggle/ -l
# Move extracted sub-folder up one (moved manually)
!mv /content/leather/new_kaggle/* /content/leather/
# Remove left-overs
!rm -r /content/leather/new_kaggle
!rm -r /content/leather/temp
# Check extracted
!ls /content/leather/ -l
# check number of files..?

"""## Convert to Train/Test Numpy Arrays"""

folding_marks = glob.glob('/content/leather/folding_marks/*.*')
grain_off = glob.glob('/content/leather/grain_off/*.*')
growth_marks = glob.glob('/content/leather/growth_marks/*.*')
loose_grains = glob.glob('/content/leather/loose_grains/*.*')
non_defective = glob.glob('/content/leather/non_defective/*.*')
pinhole = glob.glob('/content/leather/pinhole/*.*')

def load_data():

  data = []
  labels = []

  for i in folding_marks:   
      image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
      target_size= (32,32))
      image=np.array(image)
      data.append(image)
      labels.append(0)
  for i in grain_off:   
      image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
      target_size= (32,32))
      image=np.array(image)
      data.append(image)
      labels.append(1)
  for i in growth_marks:   
      image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
      target_size= (32,32))
      image=np.array(image)
      data.append(image)
      labels.append(2)
  for i in loose_grains:   
      image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
      target_size= (32,32))
      image=np.array(image)
      data.append(image)
      labels.append(3)
  for i in non_defective:   
      image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
      target_size= (32,32))
      image=np.array(image)
      data.append(image)
      labels.append(4)
  for i in pinhole:   
      image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
      target_size= (32,32))
      image=np.array(image)
      data.append(image)
      labels.append(5)

  data = np.array(data)
  labels = np.array(labels)

  return data, labels

"""## Replace with Modified_Kaggle"""

NUM_CLASSES = 100
INPUT_SHAPE = (32, 32, 3)
# REPLACE:
#(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()

#-------- ViT_03
# [176,400] = [141,120] / [35,280] = [0.8 / 0.2]
data, labels = load_data()
x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)
#-------- ViT_03

print ('check shapes: ', x_train.shape, y_train.shape, x_test.shape, y_test.shape)

"""## Configure the hyperparameters"""

# DATA
BUFFER_SIZE = 512
BATCH_SIZE = 256

# AUGMENTATION
IMAGE_SIZE = 72
PATCH_SIZE = 6
NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2

# OPTIMIZER
LEARNING_RATE = 0.001
WEIGHT_DECAY = 0.0001

# TRAINING
EPOCHS = 50

# ARCHITECTURE
LAYER_NORM_EPS = 1e-6
TRANSFORMER_LAYERS = 8
PROJECTION_DIM = 64
NUM_HEADS = 4
TRANSFORMER_UNITS = [
    PROJECTION_DIM * 2,
    PROJECTION_DIM,
]
MLP_HEAD_UNITS = [2048, 1024]

"""## data augmentation"""

data_augmentation = keras.Sequential(
    [
        layers.Normalization(),
        layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(factor=0.02),
        layers.RandomZoom(height_factor=0.2, width_factor=0.2),
    ],
    name="data_augmentation",
)
# Compute the mean and the variance of the training data for normalization.
data_augmentation.layers[0].adapt(x_train)

"""## Shifted Patch Tokenization"""

class ShiftedPatchTokenization(layers.Layer):
    def __init__(
        self,
        image_size=IMAGE_SIZE,
        patch_size=PATCH_SIZE,
        num_patches=NUM_PATCHES,
        projection_dim=PROJECTION_DIM,
        vanilla=False,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.vanilla = vanilla  # Flag to swtich to vanilla patch extractor
        self.image_size = image_size
        self.patch_size = patch_size
        self.half_patch = patch_size // 2
        self.flatten_patches = layers.Reshape((num_patches, -1))
        self.projection = layers.Dense(units=projection_dim)
        self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)

    def crop_shift_pad(self, images, mode):
        # Build the diagonally shifted images
        if mode == "left-up":
            crop_height = self.half_patch
            crop_width = self.half_patch
            shift_height = 0
            shift_width = 0
        elif mode == "left-down":
            crop_height = 0
            crop_width = self.half_patch
            shift_height = self.half_patch
            shift_width = 0
        elif mode == "right-up":
            crop_height = self.half_patch
            crop_width = 0
            shift_height = 0
            shift_width = self.half_patch
        else:
            crop_height = 0
            crop_width = 0
            shift_height = self.half_patch
            shift_width = self.half_patch

        # Crop the shifted images and pad them
        crop = tf.image.crop_to_bounding_box(
            images,
            offset_height=crop_height,
            offset_width=crop_width,
            target_height=self.image_size - self.half_patch,
            target_width=self.image_size - self.half_patch,
        )
        shift_pad = tf.image.pad_to_bounding_box(
            crop,
            offset_height=shift_height,
            offset_width=shift_width,
            target_height=self.image_size,
            target_width=self.image_size,
        )
        return shift_pad

    def call(self, images):
        if not self.vanilla:
            # Concat the shifted images with the original image
            images = tf.concat(
                [
                    images,
                    self.crop_shift_pad(images, mode="left-up"),
                    self.crop_shift_pad(images, mode="left-down"),
                    self.crop_shift_pad(images, mode="right-up"),
                    self.crop_shift_pad(images, mode="right-down"),
                ],
                axis=-1,
            )
        # Patchify the images and flatten it
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding="VALID",
        )
        flat_patches = self.flatten_patches(patches)
        if not self.vanilla:
            # Layer normalize the flat patches and linearly project it
            tokens = self.layer_norm(flat_patches)
            tokens = self.projection(tokens)
        else:
            # Linearly project the flat patches
            tokens = self.projection(flat_patches)
        return (tokens, patches)

"""## (SKIP) Visualize the patches"""

# Get a random image from the training dataset
# and resize the image
image = x_train[np.random.choice(range(x_train.shape[0]))]
resized_image = tf.image.resize( tf.convert_to_tensor([image]), size=(IMAGE_SIZE, IMAGE_SIZE))

# Vanilla patch maker: This takes an image and divides into
# patches as in the original ViT paper
(token, patch) = ShiftedPatchTokenization(vanilla=True)(resized_image / 255.0)
(token, patch) = (token[0], patch[0])
n = patch.shape[0]
count = 1
plt.figure(figsize=(4, 4))
for row in range(n):
    for col in range(n):
        plt.subplot(n, n, count)
        count = count + 1
        image = tf.reshape(patch[row][col], (PATCH_SIZE, PATCH_SIZE, 3))
        plt.imshow(image)
        plt.axis("off")
plt.show()

# Shifted Patch Tokenization: This layer takes the image, shifts it
# diagonally and then extracts patches from the concatinated images
(token, patch) = ShiftedPatchTokenization(vanilla=False)(resized_image / 255.0)
(token, patch) = (token[0], patch[0])
n = patch.shape[0]
shifted_images = ["ORIGINAL", "LEFT-UP", "LEFT-DOWN", "RIGHT-UP", "RIGHT-DOWN"]
for index, name in enumerate(shifted_images):
    print(name)
    count = 1
    plt.figure(figsize=(4, 4))
    for row in range(n):
        for col in range(n):
            plt.subplot(n, n, count)
            count = count + 1
            image = tf.reshape(patch[row][col], (PATCH_SIZE, PATCH_SIZE, 5 * 3))
            plt.imshow(image[..., 3 * index : 3 * index + 3])
            plt.axis("off")
    plt.show()

"""## patch encoding layer"""

class PatchEncoder(layers.Layer):
    def __init__(
        self, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM, **kwargs
    ):
        super().__init__(**kwargs)
        self.num_patches = num_patches
        self.position_embedding = layers.Embedding(
            input_dim=num_patches, output_dim=projection_dim
        )
        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)

    def call(self, encoded_patches):
        encoded_positions = self.position_embedding(self.positions)
        encoded_patches = encoded_patches + encoded_positions
        return encoded_patches

"""## Locality Self Attention"""

class MultiHeadAttentionLSA(tf.keras.layers.MultiHeadAttention):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # The trainable temperature term. The initial value is
        # the square root of the key dimension.
        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), trainable=True)

    def _compute_attention(self, query, key, value, attention_mask=None, training=None):
        query = tf.multiply(query, 1.0 / self.tau)
        attention_scores = tf.einsum(self._dot_product_equation, key, query)
        attention_scores = self._masked_softmax(attention_scores, attention_mask)
        attention_scores_dropout = self._dropout_layer(
            attention_scores, training=training
        )
        attention_output = tf.einsum(
            self._combine_equation, attention_scores_dropout, value
        )
        return attention_output, attention_scores

"""## Implement MLP [multi-layer perceptron]"""

def mlp(x, hidden_units, dropout_rate):
    for units in hidden_units:
        x = layers.Dense(units, activation=tf.nn.gelu)(x)
        x = layers.Dropout(dropout_rate)(x)
    return x


# Build the diagonal attention mask
diag_attn_mask = 1 - tf.eye(NUM_PATCHES)
diag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)

"""## Build the ViT"""

def create_vit_classifier(vanilla=False):
    inputs = layers.Input(shape=INPUT_SHAPE)
    # Augment data.
    augmented = data_augmentation(inputs)
    # Create patches.
    (tokens, _) = ShiftedPatchTokenization(vanilla=vanilla)(augmented)
    # Encode patches.
    encoded_patches = PatchEncoder()(tokens)

    # Create multiple layers of the Transformer block.
    for _ in range(TRANSFORMER_LAYERS):
        # Layer normalization 1.
        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
        # Create a multi-head attention layer.
        if not vanilla:
            attention_output = MultiHeadAttentionLSA(
                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1
            )(x1, x1, attention_mask=diag_attn_mask)
        else:
            attention_output = layers.MultiHeadAttention(
                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1
            )(x1, x1)
        # Skip connection 1.
        x2 = layers.Add()([attention_output, encoded_patches])
        # Layer normalization 2.
        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)
        # MLP.
        x3 = mlp(x3, hidden_units=TRANSFORMER_UNITS, dropout_rate=0.1)
        # Skip connection 2.
        encoded_patches = layers.Add()([x3, x2])

    # Create a [batch_size, projection_dim] tensor.
    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
    representation = layers.Flatten()(representation)
    representation = layers.Dropout(0.5)(representation)
    # Add MLP.
    features = mlp(representation, hidden_units=MLP_HEAD_UNITS, dropout_rate=0.5)
    # Classify outputs.
    logits = layers.Dense(NUM_CLASSES)(features)
    # Create the Keras model.
    model = keras.Model(inputs=inputs, outputs=logits)
    return model

"""## Warm-Up-Cosine??"""

# Some code is taken from:
# https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2.
class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):
    def __init__(
        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps
    ):
        super(WarmUpCosine, self).__init__()

        self.learning_rate_base = learning_rate_base
        self.total_steps = total_steps
        self.warmup_learning_rate = warmup_learning_rate
        self.warmup_steps = warmup_steps
        self.pi = tf.constant(np.pi)

    def __call__(self, step):
        if self.total_steps < self.warmup_steps:
            raise ValueError("Total_steps must be larger or equal to warmup_steps.")

        cos_annealed_lr = tf.cos(
            self.pi
            * (tf.cast(step, tf.float32) - self.warmup_steps)
            / float(self.total_steps - self.warmup_steps)
        )
        learning_rate = 0.5 * self.learning_rate_base * (1 + cos_annealed_lr)

        if self.warmup_steps > 0:
            if self.learning_rate_base < self.warmup_learning_rate:
                raise ValueError(
                    "Learning_rate_base must be larger or equal to "
                    "warmup_learning_rate."
                )
            slope = (
                self.learning_rate_base - self.warmup_learning_rate
            ) / self.warmup_steps
            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate
            learning_rate = tf.where(
                step < self.warmup_steps, warmup_rate, learning_rate
            )
        return tf.where(
            step > self.total_steps, 0.0, learning_rate, name="learning_rate"
        )

"""## Compile, train, and evaluate the model

~ RUN MODEL TEST ~
"""

def run_experiment(model):
    total_steps = int((len(x_train) / BATCH_SIZE) * EPOCHS)
    warmup_epoch_percentage = 0.10
    warmup_steps = int(total_steps * warmup_epoch_percentage)
    scheduled_lrs = WarmUpCosine(
        learning_rate_base=LEARNING_RATE,
        total_steps=total_steps,
        warmup_learning_rate=0.0,
        warmup_steps=warmup_steps,
    )

    optimizer = tfa.optimizers.AdamW(
        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY
    )

    model.compile(
        optimizer=optimizer,
        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=[
            keras.metrics.SparseCategoricalAccuracy(name="accuracy"),
            keras.metrics.SparseTopKCategoricalAccuracy(5, name="top-5-accuracy"),
        ],
    )

    # FIT THE MODEL:
    history = model.fit(x=x_train, y=y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.1,)

    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)
    print(f"Test accuracy: {round(accuracy * 100, 2)}%")
    print(f"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%")

    return history

"""## Compile and Fit 2 versions of ViT Model

* Vanilla ViT
* Shifted Patch Tokenization and Locality Self Attention modified ViT
"""

# Run experiments with the vanilla ViT
vit = create_vit_classifier(vanilla=True)
history = run_experiment(vit)

# Run experiments with the Shifted Patch Tokenization and
# Locality Self Attention modified ViT
vit_sl = create_vit_classifier(vanilla=False)
history = run_experiment(vit_sl)

"""## Store Model Training Metrics"""

import pickle

# [2] Save "Training History Data" as a pickle file
Pkl_Filename = "vit_cust06a_hist.pkl"  
with open(Pkl_Filename, 'wb') as file:  
    pickle.dump(vit, file)

# [2] Load the Pickle file
Pkl_Filename = "vit_cust06_hist.pkl"

with open(Pkl_Filename, 'rb') as file:  
    vit_cust06_hist = pickle.load(file)
vit_cust06_hist									              # loaded model
Ypredict = vit_cust06_hist.predict(x_test) 		# make prediction with loaded model

###############
## ~ ERROR ~ ##
###############
filepath = "/content/model03.h5"
# save model and architecture to single file
tf.keras.models.save_model(
    vit_sl,
    filepath,
    overwrite=True,
    include_optimizer=True,
    save_format=None,
    signatures=None,
    options=None,
    save_traces=False,
)
print("Saved model to disk")

"""## Plot train and validation curves"""

loss = history.history['loss']
v_loss = history.history['val_loss']

acc = history.history['accuracy'] 
v_acc = history.history['val_accuracy']

epochs = range(len(loss))

fig = plt.figure(figsize=(12, 4))
plt.subplot(1, 3, 1)
plt.yscale('log')
plt.plot(epochs, loss, linestyle='--', linewidth=3, color='orange', alpha=0.7, label='Train Loss')
plt.plot(epochs, v_loss, linestyle='-.', linewidth=2, color='lime', alpha=0.8, label='Valid Loss')
# plt.ylim(0.3, 100)
plt.xlabel('Epochs', fontsize=11)
plt.ylabel('Loss', fontsize=12)
plt.legend(fontsize=12)
plt.subplot(1, 3, 2)
plt.plot(epochs, acc, linestyle='--', linewidth=3, color='orange', alpha=0.7, label='Train Acc')
plt.plot(epochs, v_acc, linestyle='-.', linewidth=2, color='lime', alpha=0.8, label='Valid Acc') 
plt.xlabel('Epochs', fontsize=11)
plt.ylabel('Accuracy', fontsize=12)
plt.legend(fontsize=12)
plt.tight_layout()
# plt.savefig('/content/gdrive/My Drive/Colab Notebooks/resnet/train_acc.png', dpi=250)
plt.show()

"""## Confusion Matrix (Heat_Map)"""

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Kaggle Class Labels
class_types = ['folding_marks', 'grain_off', 'growth_marks', 'loose_grains', 'non_defective', 'pinhole']

def conf_matrix(predictions): 
    ''' Plots conf. matrix and classification report '''
    cm=confusion_matrix(y_test, np.argmax(np.round(predictions), axis=1))
    print("Classification Report:\n")
    cr=classification_report(y_test,
                                np.argmax(np.round(predictions), axis=1), 
                                target_names=[class_types[i] for i in range(len(class_types))])
    print(cr)
    plt.figure(figsize=(12,12))
    sns_hmp = sns.heatmap(cm, annot=True, xticklabels = [class_types[i] for i in range(len(class_types))], 
                yticklabels = [class_types[i] for i in range(len(class_types))], fmt="d")
    fig = sns_hmp.get_figure()
    # fig.savefig('/content/gdrive/My Drive/Colab Notebooks/resnet/heatmap.png', dpi=250)

# From Built Model
pred_class_cust06 = vit.predict(x_test)
# Create Heatmapped Confusion Matrix
conf_matrix(pred_class_cust06)

"""## [ERROR?] Make Prediction"""

## TRY SOMETHING NEW ##
import matplotlib.pyplot as plt
import PIL

# View test image (if not numpy array)
img = PIL.Image.open(x_test[:1])#'/content/games.png')
plt.imshow(img)

# Generate a prediction using model.predict() 
print("Generate a prediction")
prediction = vit.predict(x_test[:1])

# Show it's shape:
print("prediction shape:", prediction.shape)

# Kaggle Class Labels
class_types = ['folding_marks', 'grain_off', 'growth_marks', 'loose_grains', 'non_defective', 'pinhole']

print("type " + str(type(x_test)))                       # View test type?
print("shape " + str(x_test.shape))                     # View test shape?

# Prediction 1:
pred_class1 = vit.predict(x_test)

# Display prediction
prediction1 = np.argmax(pred_class1)
print('predicted class', prediction1)                   # print preditions
#print("Predicted Class " + class_types[prediction])    # ERROR?

# Prediction 2:
pred_class2 = vit_sl.predict(x_test)                    # Get features test image

# Display prediction
prediction2 = np.argmax(pred_class2)
print('predicted class', prediction2)                   # print preditions
#print("Predicted Class " + class_types[prediction])    # ERROR?