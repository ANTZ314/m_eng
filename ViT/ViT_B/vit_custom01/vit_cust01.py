# -*- coding: utf-8 -*-
"""vit_cust01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rwJUWsGLvWknM9WBQ-iLLOF1eoXbZkw5

# ViT Model with Custom Leather Dataset

**NOTES**
* ViT Code taken from **ViT_3** & **ViT_4a**
* Kaggle dataset - 6 folders of 600 images each @ [224, 224, 3]
* Kaggle new     - 6 folders of 29,400 imgs each @ [32, 32, 3]

**STEPS:**
* Connect to G_Drive & Copy compressed dataset file (~174Mb)
* Extract the file (.tar / .zip) to new directory
* Begin ViT processing...

## G_Drive & Dataset
"""

from google.colab import drive
drive.mount('/content/gdrive')

# Check dataset avaiabe
!ls "/content/gdrive/MyDrive/dataset/"

# Copy Kaggle Leather Dataset to current directory
!cp /content/gdrive/MyDrive/dataset/new_kaggle.tar.xz .

!ls /content/ -l

# Ctreate dataset destination
!mkdir /content/leather
# Extract to new direcotry
#!tar -xvf  'new_kaggle.tar.xz' -C '/content/leather/'
!tar -xvf  'new_kaggle.tar.xz' -C '/content/leather/'

!ls /content/leather/new_kaggle/ -l
# Move extracted sub-folder up one (moved manually)
!mv /content/leather/new_kaggle/* /content/leather/
# Remove left-overs
!rm -r /content/leather/new_kaggle
!rm -r /content/leather/temp
# Check extracted
!ls /content/leather/ -l
# check number of files..?

"""## Combine dataset into single sub-directories
```
/content/leather/
|--folding_marks (600 -> 29,400 images)
|--growth_marks (600 -> 29,400  images)
|--loose_grains ((600 -> 29,400  images)
|--non_defective (600 -> 29,400  images)
|--pinhole (600 -> 29,400  images)
```
"""

# [SKIP] Create single dataset to then split
!mkdir leather

!cp -r /content/kaggle/train/* /content/leather/

!cp -r /content/kaggle/validate/folding_marks/* /content/leather/folding_marks/
!cp -r /content/kaggle/validate/grain_off/* /content/leather/grain_off/
!cp -r /content/kaggle/validate/growth_marks/* /content/leather/growth_marks/
!cp -r /content/kaggle/validate/loose_grains/* /content/leather/loose_grains/
!cp -r /content/kaggle/validate/non_defective/* /content/leather/non_defective/
!cp -r /content/kaggle/validate/pinhole/* /content/leather/pinhole/

!ls /content/leather -l

"""## Load datset into train/test & reshape

Load the dataset images into a numpy array format
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split

# Kaggle Class Labels
class_types = ['folding_marks', 'grain_off', 'growth_marks', 'loose_grains', 'non_defective', 'pinhole']

import glob
folding_marks = glob.glob('/content/leather/folding_marks/*.*')
grain_off = glob.glob('/content/leather/grain_off/*.*')
growth_marks = glob.glob('/content/leather/growth_marks/*.*')
loose_grains = glob.glob('/content/leather/loose_grains/*.*')
non_defective = glob.glob('/content/leather/non_defective/*.*')
pinhole = glob.glob('/content/leather/pinhole/*.*')

def load_data():

  data = []
  labels = []

  for i in folding_marks:   
      image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
      target_size= (32,32))
      image=np.array(image)
      data.append(image)
      labels.append(0)
  for i in grain_off:   
      image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
      target_size= (32,32))
      image=np.array(image)
      data.append(image)
      labels.append(1)
  for i in growth_marks:   
      image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
      target_size= (32,32))
      image=np.array(image)
      data.append(image)
      labels.append(2)
  for i in loose_grains:   
      image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
      target_size= (32,32))
      image=np.array(image)
      data.append(image)
      labels.append(3)
  for i in non_defective:   
      image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
      target_size= (32,32))
      image=np.array(image)
      data.append(image)
      labels.append(4)
  for i in pinhole:   
      image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
      target_size= (32,32))
      image=np.array(image)
      data.append(image)
      labels.append(5)

  data = np.array(data)
  labels = np.array(labels)

  return data, labels

data, labels = load_data()

X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2,
                                                random_state=42)

print(f"X_train shape: {X_train.shape} - y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape} - y_test shape: {y_test.shape}")

"""## ViT Model Begin:

Attempt **ViT_3**

Also utilise pretrained model...
"""

print(class_types)
print('check shapes: ', X_train.shape, y_train.shape, X_test.shape, y_test.shape)

# train_im, test_im = X_train/255.0 , x_test/255.0
train_lab_categorical = tf.keras.utils.to_categorical(y_train, num_classes=6, dtype='uint8')
test_lab_categorical = tf.keras.utils.to_categorical(y_test, num_classes=6, dtype='uint8')


train_im, valid_im, train_lab, valid_lab = train_test_split(X_train, train_lab_categorical, test_size=0.20, 
                                                            stratify=train_lab_categorical, 
                                                            random_state=42, shuffle = True) # stratify is unncessary 

print("train data shape after the split: ", train_im.shape)
print('new validation data shape: ', valid_im.shape)
print("validation labels shape: ", valid_lab.shape)

#print('train im and label types: ', type(train_im), type(train_lab))

training_data = tf.data.Dataset.from_tensor_slices((train_im, train_lab))
validation_data = tf.data.Dataset.from_tensor_slices((valid_im, valid_lab))
test_data = tf.data.Dataset.from_tensor_slices((X_test, test_lab_categorical))

print("test data shape: ", X_test.shape)

#print('check types; ', type(training_data), type(validation_data))

autotune = tf.data.AUTOTUNE 

# Set buffer sizes to match shape outputs:
train_data_batches = training_data.shuffle(buffer_size=2304).batch(128).prefetch(buffer_size=autotune)
valid_data_batches = validation_data.shuffle(buffer_size=576).batch(32).prefetch(buffer_size=autotune)
test_data_batches = test_data.shuffle(buffer_size=720).batch(32).prefetch(buffer_size=autotune)

"""## Patch Generation

Divides images into patches of given patch size
"""

import tensorflow as tf
from tensorflow.keras import layers

#==========================#
### generate patches 
#==========================#
class generate_patch(layers.Layer):
  def __init__(self, patch_size):
    super(generate_patch, self).__init__()
    self.patch_size = patch_size
    
  def call(self, images):
    batch_size = tf.shape(images)[0]
    patches = tf.image.extract_patches(images=images, 
                                       sizes=[1, self.patch_size, self.patch_size, 1], 
                                       strides=[1, self.patch_size, self.patch_size, 1], rates=[1, 1, 1, 1], padding="VALID")
    patch_dims = patches.shape[-1]
    patches = tf.reshape(patches, [batch_size, -1, patch_dims]) #here shape is (batch_size, num_patches, patch_h*patch_w*c) 
    return patches

"""## Helper function to visualize the patches (SKIP)"""

import matplotlib.pyplot as plt
from itertools import islice, count

train_iter_7im, train_iter_7label = next(islice(training_data, 7, None)) # access the 7th element from the iterator


train_iter_7im = tf.expand_dims(train_iter_7im, 0)
train_iter_7label = train_iter_7label.numpy() 

print('check shapes: ', train_iter_7im.shape) 

patch_size=4 
######################
# num patches (W * H) /P^2 where W, H are from original image, P is patch dim. 
# Original image (H * W * C), patch N * P*P *C, N num patches
######################
generate_patch_layer = generate_patch(patch_size=patch_size)
patches = generate_patch_layer(train_iter_7im)

print ('patch per image and patches shape: ', patches.shape[1], '\n', patches.shape)


def render_image_and_patches(image, patches):
    plt.figure(figsize=(6, 6))
    plt.imshow(tf.cast(image[0], tf.uint8))
    plt.xlabel(class_types [np.argmax(train_iter_7label)], fontsize=13)
    n = int(np.sqrt(patches.shape[1]))
    plt.figure(figsize=(6, 6))
    #plt.suptitle(f"Image Patches", size=13)
    for i, patch in enumerate(patches[0]):
        ax = plt.subplot(n, n, i+1)
        patch_img = tf.reshape(patch, (patch_size, patch_size, 3))
        ax.imshow(patch_img.numpy().astype("uint8"))
        ax.axis('off')    


render_image_and_patches(train_iter_7im, patches)

"""## Positonal Encoding Layer"""

class PatchEncode_Embed(layers.Layer):
  """
  2 steps happen here:
  	1. flatten the patches
  	2. Map to dim D; patch embeddings
  """

  def __init__(self, num_patches, projection_dim):
    super(PatchEncode_Embed, self).__init__()
    self.num_patches = num_patches
    self.projection = layers.Dense(units=projection_dim)
    self.position_embedding = layers.Embedding(
    input_dim=num_patches, output_dim=projection_dim)


  def call(self, patch):
    positions = tf.range(start=0, limit=self.num_patches, delta=1)
    encoded = self.projection(patch) +               self.position_embedding(positions)
    return encoded

"""## Patch Generation & Positional Encoding:

This part:
* takes images as inputs,  
* Conv layer filter matches query dim of multi-head attention layer
* Add embeddings by randomly initializing the weights
"""

def generate_patch_conv_orgPaper_f(patch_size, hidden_size, inputs):
  patches = layers.Conv2D(filters=hidden_size, kernel_size=patch_size, strides=patch_size, padding='valid')(inputs)
  row_axis, col_axis = (1, 2) # channels last images
  seq_len = (inputs.shape[row_axis] // patch_size) * (inputs.shape[col_axis] // patch_size)
  x = tf.reshape(patches, [-1, seq_len, hidden_size])
  return x

"""## Positonal Encoding Layer"""

class AddPositionEmbs(layers.Layer):
  """inputs are image patches 
  Custom layer to add positional embeddings to the inputs."""

  def __init__(self, posemb_init=None, **kwargs):
    super().__init__(**kwargs)
    self.posemb_init = posemb_init
    #posemb_init=tf.keras.initializers.RandomNormal(stddev=0.02), name='posembed_input') # used in original code

  def build(self, inputs_shape):
    pos_emb_shape = (1, inputs_shape[1], inputs_shape[2])
    self.pos_embedding = self.add_weight('pos_embedding', pos_emb_shape, initializer=self.posemb_init)

  def call(self, inputs, inputs_positions=None):
    # inputs.shape is (batch_size, seq_len, emb_dim).
    pos_embedding = tf.cast(self.pos_embedding, inputs.dtype)

    return inputs + pos_embedding

pos_embed_layer = AddPositionEmbs(posemb_init=tf.keras.initializers.RandomNormal(stddev=0.02))

"""## Transformer Encoder Block:

part of ViT Implementation:
This block implements the Transformer Encoder Block

Contains 3 parts--
1. LayerNorm 
2. Multi-Layer Perceptron 
3. Multi-Head Attention

For repeating the Transformer Encoder Block we use Encoder_f function. 
"""

def mlp_block_f(mlp_dim, inputs):
  x = layers.Dense(units=mlp_dim, activation=tf.nn.gelu)(inputs)
  x = layers.Dropout(rate=0.1)(x) # dropout rate is from original paper,
  x = layers.Dense(units=inputs.shape[-1], activation=tf.nn.gelu)(x) # check GELU paper
  x = layers.Dropout(rate=0.1)(x)
  return x

def Encoder1Dblock_f(num_heads, mlp_dim, inputs):
  x = layers.LayerNormalization(dtype=inputs.dtype)(inputs)
  x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1], dropout=0.1)(x, x) 
  # self attention multi-head, dropout_rate is from original implementation
  x = layers.Add()([x, inputs]) # 1st residual part 
  
  y = layers.LayerNormalization(dtype=x.dtype)(x)
  y = mlp_block_f(mlp_dim, y)
  y_1 = layers.Add()([y, x]) #2nd residual part 
  return y_1

def Encoder_f(num_layers, mlp_dim, num_heads, inputs):
  x = AddPositionEmbs(posemb_init=tf.keras.initializers.RandomNormal(stddev=0.02), name='posembed_input')(inputs)
  x = layers.Dropout(rate=0.2)(x)
  for _ in range(num_layers):
    x = Encoder1Dblock_f(num_heads, mlp_dim, x)

  encoded = layers.LayerNormalization(name='encoder_norm')(x)
  return encoded

"""# VISION TRANSFORMER MAIN:

Building blocks of ViT:

Check other gists or the complete notebook []

* Patches (generate_patch_conv_orgPaper_f) + embeddings (within Encoder_f)
* Transformer Encoder Block (Encoder_f)
* Final Classification

### Hyperparameters
"""

transformer_layers = 6
patch_size = 4
hidden_size = 64
num_heads = 4
mlp_dim = 128

rescale_layer = tf.keras.Sequential([layers.experimental.preprocessing.Rescaling(1./255)])

"""## Build the ViT Model:"""

def build_ViT():
  inputs = layers.Input(shape=train_im.shape[1:])
  # rescaling (normalizing pixel val between 0 and 1)
  rescale = rescale_layer(inputs)
  # generate patches with conv layer
  patches = generate_patch_conv_orgPaper_f(patch_size, hidden_size, rescale)

  #===================================#
  # ready for the transformer blocks
  #===================================#
  encoder_out = Encoder_f(transformer_layers, mlp_dim, num_heads, patches)  

  #===================================#
  #  final part (mlp to classification)
  #===================================#
  #encoder_out_rank = int(tf.experimental.numpy.ndim(encoder_out))
  im_representation = tf.reduce_mean(encoder_out, axis=1)  # (1,) or (1,2)
  # similar to the GAP, this is from original Google GitHub

  logits = layers.Dense(units=len(class_types), name='head', kernel_initializer=tf.keras.initializers.zeros)(im_representation) # !!! important !!! activation is linear 

  final_model = tf.keras.Model(inputs = inputs, outputs = logits)
  return final_model

ViT_model = build_ViT()
#ViT_model.summary()

"""## MAKE PREDICTION ON TEST IMAGE ARRAY?

**ERROR** - Resources exhausted
Split [224, 224, 3] images to [32,32,3] = (7x)
"""

# [SKIP] [NOT CORRECT INTERPRETATION] Display prediction
# Can't use X_test for prediction - Entire array of test images

print(type(X_test))
# 'x_test' is loaded same time as training data
pred_class = ViT_model.predict(X_test)                  # Get features test image

#print(f"Uninterpreted: {}".pred_class)                 # view 1000 model features
prediction = np.argmax(pred_class)
print('Predicted Class', prediction)                    # print preditions
#print("Predicted Class " + class_types[prediction])     #

"""# Using ViT_4a's Model Analysis:"""

# Required Add-Ons:
!pip install tensorflow-addons

"""## Model - Compile, Reduce & FIT

1h33 to Train
Test accuracy: 91.77%
Test top 5 accuracy: 100.0%
"""

from tensorflow import keras
#from tensorflow.keras import layers
import tensorflow_addons as tfa

# Define parameters
learning_rate = 0.001
weight_decay = 0.0001
batch_size = 256
num_epochs = 100

def run_experiment(model):
    optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)

    model.compile(
        optimizer=optimizer,
        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=[
            keras.metrics.SparseCategoricalAccuracy(name="accuracy"),
            keras.metrics.SparseTopKCategoricalAccuracy(5, name="top-5-accuracy"),
        ],
    )

    checkpoint_filepath = "/tmp/checkpoint"
    checkpoint_callback = keras.callbacks.ModelCheckpoint(
        checkpoint_filepath,
        monitor="val_accuracy",
        save_best_only=True,
        save_weights_only=True,
    )

    history = model.fit(
        x=X_train,
        y=y_train,
        batch_size=batch_size,
        epochs=num_epochs,
        validation_split=0.1,
        callbacks=[checkpoint_callback],
    )

    model.load_weights(checkpoint_filepath)
    _, accuracy, top_5_accuracy = model.evaluate(X_test, y_test)
    print(f"Test accuracy: {round(accuracy * 100, 2)}%")
    print(f"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%")

    return history

history = run_experiment(ViT_model)

"""## Store Trained Model"""

import pickle

# [2] Save "Training History Data" as a pickle file
Pkl_Filename = "ViT_Cust01_Model.pkl"  
with open(Pkl_Filename, 'wb') as file:  
    pickle.dump(history, file)

"""## [FAIL] New Test Image"""

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# Image (224x224) - Drag'n'Drop
new_test = '/content/test.jpg'

img = mpimg.imread(X_test)
imgplot = plt.imshow(img)
plt.show()

"""## Plot Training Data"""

# [2] Load the "Training History Data" Pickle file
#with open(Pkl_Filename, 'rb') as file:  
#    Pickled_LR_Model = pickle.load(file)
#Pickled_LR_Model									              # loaded model
#print(Pickled_LR_Model.accuracy)

# list all data in history
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()